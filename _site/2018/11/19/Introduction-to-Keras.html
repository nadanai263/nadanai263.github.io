

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Introduction to Keras</title>
    <meta name="description" content="Keras is a high-level wrapper for Tensorflow and Theano, both of which are powerful machine learning frameworks, and for many people it is their first contact with deep learning. In this first tutorial, I present the most basic introduction to Keras, and develop a code framework which we'll recycle in later tutorials covering more advanced topics.">
    <meta name="author" content="Nadanai Laohakunakorn">

    <!-- Enable responsive viewport -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="/assets/themes/twitter/bootstrap/css/bootstrap.2.2.2.min.css" rel="stylesheet">  
  <!--    <link href="/assets/themes/twitter/bootstrap/css/cosmo.css" rel="stylesheet"> -->
    <link href="/assets/themes/twitter/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">
 <!--   <link href="/assets/themes/twitter/css/kbroman.css" rel="stylesheet" type="text/css" media="all">   -->
    <link href="/assets/themes/twitter/css/nl.css" rel="stylesheet" type="text/css" media="all">

    <!-- Le fav and touch icons -->

    <!-- atom & rss feed -->
    <link href="nil" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
    <link href="nil" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">

  </head>

  <body>
    <div class="navbar">
      <div class="navbar-inner">
        <div class="container-narrow">
          <a class="brand" href="/">Nadanai Laohakunakorn</a>
          <ul class="nav">
            <li><a href="/pages/tutorials.html">Tutorials</a></li>
            <li><a href="/pages/algorithms.html">Algorithms</a></li>
            <li><a href="/pages/CV.html">CV</a></li>
        </div>
      </div>
    </div>

    <div class="container-narrow">

      <div class="content">
        

<div class="page-header">
  <h1>Introduction to Keras </h1>
</div>

<div class="row-fluid post-full">
  <div class="span12">
    <div class="date">
      <span>19 November 2018</span>
    </div>
    <div class="content">
      <p>Keras is a high-level wrapper for Tensorflow and Theano, both of which are powerful machine learning frameworks, and for many people it is their first contact with deep learning. In this first tutorial, I present the most basic introduction to Keras, and develop a code framework which we’ll recycle in later tutorials covering more advanced topics.</p>

<p>Keras provides a quick and easy way to define and train neural network models. It does this by wrapping around powerful frameworks such as Tensorflow and Theano, which are optimised to carry out the linear algebraic computations necessary for machine learning. We won’t go into details or theory here; that’s the subject of very many excellent tutorials around the web. In this tutorial, I will present the bare-bones code necessary to get a model up and running. We’ll iterate over this code framework in subsequent tutorials. But first, let’s do some setting up.</p>

<p>The tutorial contains the following sections:</p>

<ul id="markdown-toc">
  <li><a href="#1-getting-started" id="markdown-toc-1-getting-started">1. Getting started</a></li>
  <li><a href="#2-loading-data" id="markdown-toc-2-loading-data">2. Loading data</a></li>
  <li><a href="#3-basic-framework-for-using-keras" id="markdown-toc-3-basic-framework-for-using-keras">3. Basic framework for using Keras</a>    <ul>
      <li><a href="#31-defining-the-model" id="markdown-toc-31-defining-the-model">3.1 Defining the model</a></li>
      <li><a href="#32-compiling-the-model" id="markdown-toc-32-compiling-the-model">3.2 Compiling the model</a></li>
      <li><a href="#33-training" id="markdown-toc-33-training">3.3 Training</a></li>
      <li><a href="#34-evaluate-our-results" id="markdown-toc-34-evaluate-our-results">3.4 Evaluate our results</a></li>
    </ul>
  </li>
  <li><a href="#4-useful-functions-for-keras-workflow" id="markdown-toc-4-useful-functions-for-keras-workflow">4. Useful functions for Keras workflow</a>    <ul>
      <li><a href="#41-saving-and-loading-models" id="markdown-toc-41-saving-and-loading-models">4.1 Saving and loading models</a></li>
      <li><a href="#42-plotting-training-history" id="markdown-toc-42-plotting-training-history">4.2 Plotting training history</a></li>
      <li><a href="#43-cross-validation-using-a-dev-set" id="markdown-toc-43-cross-validation-using-a-dev-set">4.3 Cross-validation using a dev set</a></li>
    </ul>
  </li>
  <li><a href="#5-conclusions" id="markdown-toc-5-conclusions">5. Conclusions</a></li>
</ul>
<!-- toc -->

<h3 id="1-getting-started">1. Getting started</h3>
<p>First, we need the following libraries: <code class="highlighter-rouge">keras</code>, <code class="highlighter-rouge">python-mnist</code>, <code class="highlighter-rouge">numpy</code>.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install keras python-mnist numpy
</code></pre></div></div>

<h3 id="2-loading-data">2. Loading data</h3>

<p>Loading and preparing data is a big topic which is covered in more detail elsewhere. Today we will use the <code class="highlighter-rouge">python-mnist</code> package and load the MNIST dataset, which is a standard ‘hello-world’-type dataset used as a reference for deep learning models. It consists of 60,000 training and 10,000 test examples of 28x28 images of handwritten digits; the objective of the model is to predict which digit an image corresponds to. It is thus a 10-class classification problem.</p>

<p>First we load all images into memory as 28x28 numpy arrays. These 8-bit images are then normalised by dividing by 255 to put their values between 0 and 1, and then flattened into 1D vectors of length 784. Finally, we convert the categorical (y) labels to one-hot binary class matrices. The following code block does this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">NUM_CLASSES</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c"># Set seed for reproducible results</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># 1. Load data using MNIST helper functions </span>
<span class="c"># and convert to flattened, normalised numpy arrays</span>

<span class="n">mndata</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="s">'./MNIST_data'</span><span class="p">,</span><span class="n">gz</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">images_train</span><span class="p">,</span> <span class="n">labels_train</span> <span class="o">=</span> <span class="n">mndata</span><span class="o">.</span><span class="n">load_training</span><span class="p">()</span>
<span class="n">images_test</span><span class="p">,</span> <span class="n">labels_test</span> <span class="o">=</span> <span class="n">mndata</span><span class="o">.</span><span class="n">load_testing</span><span class="p">()</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">images_train</span><span class="p">)</span><span class="o">/</span><span class="mi">255</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">labels_train</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">images_test</span><span class="p">)</span><span class="o">/</span><span class="mi">255</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">labels_test</span><span class="p">)</span>

<span class="c"># 2. Convert class vectors from digit labels to binary class matrices (N x 10)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">NUM_CLASSES</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">NUM_CLASSES</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s">'Number of training samples'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s">'Number of test samples'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'x_train shape:'</span><span class="p">,</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y_train shape:'</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'x_test shape:'</span><span class="p">,</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y_test shape:'</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>60000 Number of training samples
10000 Number of test samples
x_train shape: (60000, 784)
y_train shape: (60000, 10)
x_test shape: (10000, 784)
y_test shape: (10000, 10)
</code></pre></div></div>

<p>The next part shows an example of the original image, a flattened image, the original label, and finally the one-hot representation of the label, just so we have a glimpse into what the data looks like.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">index</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Example image:'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">));</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Flattened image:'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mf">0.2</span><span class="p">));</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">784</span><span class="p">),</span> <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Corresponding label:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">labels_train</span><span class="p">)[</span><span class="n">index</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">'One-hot label:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Example image:
</code></pre></div></div>

<p><img src="/assets/images/IK_6_1.png" width="150" alt="me" align="center" hspace="40" vspace="40" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Flattened image:
</code></pre></div></div>

<p><img src="/assets/images/IK_6_3.png" width="400" alt="me" align="center" hspace="40" vspace="40" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Corresponding label:
2
One-hot label:
[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
</code></pre></div></div>

<h3 id="3-basic-framework-for-using-keras">3. Basic framework for using Keras</h3>

<p>Here is where the Keras part begins. At its simplest, the four steps we need to operate Keras are:</p>

<ol>
  <li>Define the model graph</li>
  <li>Compile the model</li>
  <li>Train the model</li>
  <li>Evaluate our results</li>
</ol>

<p>These are similar to the steps in Tensorflow, but with fewer intricacies at each step.</p>

<h4 id="31-defining-the-model">3.1 Defining the model</h4>

<p>There are two ways to define a model in Keras, using either a Sequential or a Functional approach. The Sequential approach generates the model layer-by-layer, while in the Functional approach you define how the layers are linked. It’s thus easy to make simple models using the Sequential approach, but more complex models would require a Functional definition. Here I show how to make a one-layer neural network which takes the input vector, fully connects it to a single 10-neuron layer, and applies a softmax activation to generate the output.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Define model graph using Sequential API</span>
 
<span class="n">model_name</span> <span class="o">=</span> <span class="s">'FC1'</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">NUM_CLASSES</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'D1'</span><span class="p">))</span> <span class="c"># Add first dense layer</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s">'Output'</span><span class="p">))</span> <span class="c"># Add softmax activation </span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
D1 (Dense)                   (None, 10)                7850      
_________________________________________________________________
Output (Activation)          (None, 10)                0         
=================================================================
Total params: 7,850
Trainable params: 7,850
Non-trainable params: 0
_________________________________________________________________
</code></pre></div></div>

<p>First, we define <code class="highlighter-rouge">model</code> to be a <code class="highlighter-rouge">Sequential()</code> object. Then, we merely add layers to this object. The first layer requires the input dimensions to be specified. Each layer is given a name; normally I always name my layers, as this is useful for saving and loading models and weights (shown below). Finally, the structure of the model can be shown using the <code class="highlighter-rouge">summary()</code> method.</p>

<p>The following shows how to generate the same model using the Functional approach. As you can see, here you define connections between layers such as Input, Dense, and Activation; then, you build the model using the <code class="highlighter-rouge">Model()</code> call. As before, the Input layer requires you to specify the input dimensions; each layer is again given a name. While this approach looks slightly more complicated, it’s my preferred approach for building models in Keras as it offers much more flexibility.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Define model graph using Functional API</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s">'FC1'</span>
<span class="n">x_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],),</span> <span class="n">name</span><span class="o">=</span><span class="s">'Input1'</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">NUM_CLASSES</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'D1'</span><span class="p">)(</span><span class="n">x_input</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'Output'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">x_input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input1 (InputLayer)          (None, 784)               0         
_________________________________________________________________
D1 (Dense)                   (None, 10)                7850      
_________________________________________________________________
Output (Activation)          (None, 10)                0         
=================================================================
Total params: 7,850
Trainable params: 7,850
Non-trainable params: 0
_________________________________________________________________
</code></pre></div></div>

<h4 id="32-compiling-the-model">3.2 Compiling the model</h4>

<p>With our model built, we proceed to compile it. Here, the compilation requires you to define the loss function and optimiser, just like in Tensorflow. The loss function determines how far the model predictions are from the training labels; the optimiser determines how you minimise that loss. Finally, the metrics argument specifies what quantities you evaluate during training and testing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Compile the model</span>
<span class="n">adam</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">categorical_crossentropy</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">adam</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</code></pre></div></div>

<h4 id="33-training">3.3 Training</h4>

<p>Finally, we train the model. To do this, we call the <code class="highlighter-rouge">fit()</code> method, which returns a <code class="highlighter-rouge">history</code> object containing a record of the training process, which we’ll be able to access later. For our first basic example, we will feed in all the training data. The training proceeds using a minibatch gradient descent, where each batch has 32 examples. We train for 10 epochs, or passes through the complete dataset. Finally, the verbose argument turns on the console output of the training process.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Train the model</span>
<span class="n">history</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 1/10
60000/60000 [==============================] - 4s 59us/step - loss: 0.4678 - acc: 0.8776
Epoch 2/10
60000/60000 [==============================] - 3s 56us/step - loss: 0.3044 - acc: 0.9160
Epoch 3/10
60000/60000 [==============================] - 3s 45us/step - loss: 0.2834 - acc: 0.9203
Epoch 4/10
60000/60000 [==============================] - 3s 43us/step - loss: 0.2734 - acc: 0.9227
Epoch 5/10
60000/60000 [==============================] - 3s 43us/step - loss: 0.2665 - acc: 0.9262
Epoch 6/10
60000/60000 [==============================] - 3s 52us/step - loss: 0.2618 - acc: 0.9271
Epoch 7/10
60000/60000 [==============================] - 4s 63us/step - loss: 0.2584 - acc: 0.9281
Epoch 8/10
60000/60000 [==============================] - 3s 53us/step - loss: 0.2554 - acc: 0.9295
Epoch 9/10
60000/60000 [==============================] - 3s 53us/step - loss: 0.2527 - acc: 0.9303
Epoch 10/10
60000/60000 [==============================] - 3s 54us/step - loss: 0.2505 - acc: 0.9311
</code></pre></div></div>

<h4 id="34-evaluate-our-results">3.4 Evaluate our results</h4>

<p>Once our model is trained, we can evaluate the model on our test data: the <code class="highlighter-rouge">evaluate()</code> method returns loss and accuracy scores:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Evaluate</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Test Loss:'</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Test Accuracy:'</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>10000/10000 [==============================] - 0s 33us/step
Test Loss: 0.2638074106231332
Test Accuracy: 0.9281
</code></pre></div></div>

<p>We see that our most basic 10-node network, containing 7,850 parameters, performs with 92% accuracy on the held-out test set: this is the expected standard result, and is far from the maximum possible <a href="https://www.kaggle.com/c/digit-recognizer/discussion/61480#latest-427248">state-of-the-art</a> value of 99.8%.</p>

<h3 id="4-useful-functions-for-keras-workflow">4. Useful functions for Keras workflow</h3>

<p>I hope you’ve seen from the discussion above how easy it is to build, train, and test a model using Keras. As the problem and models become more complex, we will need to introduce tricks at each step, but the basic framework will remain unchanged. In the rest of this tutorial I will show some functions which will already be useful for our current approach:</p>

<ul>
  <li>Saving and loading models: essential tasks</li>
  <li>Plotting training history: useful to visualise and debug the training process</li>
  <li>Automatic cross-validation: an out-of-the-box method to perform cross-validation with a dev set. More sophisticated techniques will be discussed elsewhere.</li>
</ul>

<h4 id="41-saving-and-loading-models">4.1 Saving and loading models</h4>

<p>Once we have trained our model, we’d like to save both the architecture and weights. The architecture is saved as a json file, while the weights are saved as an hdf5 file.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Saving model and weights</span>
<span class="n">DIR</span> <span class="o">=</span> <span class="s">'./Keras_results/'</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">DIR</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">DIR</span><span class="p">)</span>

<span class="c"># 1. Save model architecture as json</span>
<span class="n">json_string</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to_json</span><span class="p">()</span>
<span class="nb">open</span><span class="p">(</span><span class="n">DIR</span> <span class="o">+</span> <span class="n">model_name</span> <span class="o">+</span> <span class="s">'_arch.json'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json_string</span><span class="p">)</span>

<span class="c"># 2. Save model weights as hdf5</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="n">DIR</span> <span class="o">+</span> <span class="n">model_name</span> <span class="o">+</span> <span class="s">'_weights.h5'</span><span class="p">)</span>
</code></pre></div></div>

<p>To load a model we can run the steps in reverse: load the architecture from a json file, and load the weights from an h5 file. We can alternatively rebuild the model as shown above, and then feed that directly with the weights. The weights are assigned to the correct layers if the argument <code class="highlighter-rouge">by_name=True</code> is used.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Loading model and weights</span>

<span class="c"># 1. Load model architecture from json</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">model_from_json</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">DIR</span> <span class="o">+</span> <span class="n">model_name</span> <span class="o">+</span> <span class="s">'_arch.json'</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>

<span class="c"># 2. Load model weights from hdf5</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">DIR</span> <span class="o">+</span> <span class="n">model_name</span> <span class="o">+</span> <span class="s">'_weights.h5'</span><span class="p">,</span> <span class="n">by_name</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="42-plotting-training-history">4.2 Plotting training history</h4>

<p>We can interrogate the history object returned by the <code class="highlighter-rouge">fit()</code> method to plot our training history:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fig, ax = plt.subplots(1, 2, figsize=(12,4));
ax[0].set_title('Loss'); ax[0].set_xlabel('Epochs');
ax[0].plot(history.epoch, history.history["loss"], label="Train loss");
ax[1].set_title('Accuracy'); ax[1].set_xlabel('Epochs');
ax[1].plot(history.epoch, history.history["acc"], label="Train accuracy");
ax[0].legend();
ax[1].legend();
</code></pre></div></div>

<p><img src="/assets/images/IK_33_0.png" width="75%" alt="me" align="center" hspace="40" vspace="40" /></p>

<h4 id="43-cross-validation-using-a-dev-set">4.3 Cross-validation using a dev set</h4>

<p>While cross-validation will be discussed in more detail in another tutorial, Keras already offers an automatic way to train with cross-validation, using the <code class="highlighter-rouge">validation_split</code> argument of the <code class="highlighter-rouge">fit()</code> method. The value tells Keras to hold out a proportion of the training set and use that as a dev set, allowing us to estimate our model’s predictive performance before it sees the held-out test set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Train the model with cross-validation</span>
<span class="n">history</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Train on 54000 samples, validate on 6000 samples
Epoch 1/10
54000/54000 [==============================] - 2s 46us/step - loss: 0.4885 - acc: 0.8712 - val_loss: 0.2728 - val_acc: 0.9265
Epoch 2/10
54000/54000 [==============================] - 3s 55us/step - loss: 0.3138 - acc: 0.9125 - val_loss: 0.2450 - val_acc: 0.9343
Epoch 3/10
54000/54000 [==============================] - 2s 41us/step - loss: 0.2916 - acc: 0.9183 - val_loss: 0.2347 - val_acc: 0.9352
Epoch 4/10
54000/54000 [==============================] - 2s 45us/step - loss: 0.2803 - acc: 0.9217 - val_loss: 0.2291 - val_acc: 0.9367
Epoch 5/10
54000/54000 [==============================] - 2s 45us/step - loss: 0.2731 - acc: 0.9234 - val_loss: 0.2278 - val_acc: 0.9388
Epoch 6/10
54000/54000 [==============================] - 2s 46us/step - loss: 0.2683 - acc: 0.9249 - val_loss: 0.2276 - val_acc: 0.9377
Epoch 7/10
54000/54000 [==============================] - 3s 47us/step - loss: 0.2644 - acc: 0.9262 - val_loss: 0.2230 - val_acc: 0.9387
Epoch 8/10
54000/54000 [==============================] - 3s 53us/step - loss: 0.2610 - acc: 0.9274 - val_loss: 0.2234 - val_acc: 0.9403
Epoch 9/10
54000/54000 [==============================] - 3s 52us/step - loss: 0.2582 - acc: 0.9285 - val_loss: 0.2233 - val_acc: 0.9397
Epoch 10/10
54000/54000 [==============================] - 3s 63us/step - loss: 0.2563 - acc: 0.9291 - val_loss: 0.2228 - val_acc: 0.9403
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Plot training history </span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">));</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">);</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">"loss"</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">"Train loss"</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">"val_loss"</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">"Validation loss"</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">);</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">"acc"</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">"Train accuracy"</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">"val_acc"</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">"Validation accuracy"</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</code></pre></div></div>

<p><img src="/assets/images/IK_37_0.png" width="75%" alt="me" align="center" hspace="40" vspace="40" /></p>

<h3 id="5-conclusions">5. Conclusions</h3>

<p>In this tutorial I showed a basic framework in Keras, where you define, compile, train, and test a model on a toy dataset. The basic framework remains unchanged even as the problem and model complexity increases. I hope to show more things you can do with Keras soon.</p>

<p>A jupyter notebook is available for this tutorial on my <a href="https://github.com/nadanai263/datasciportfolio">GitHub repository</a>. The definitive guide to Keras is the documentation available on their <a href="https://keras.io">homepage</a>.</p>

    </div>

    

    

    <hr>
    <div class="pagination">
      <ul>
      
        <li class="prev"><a href="/2018/11/15/Getting-LinkedIn-Data-with-Python" title="Getting LinkedIn data with Python">&larr; Previous</a></li>
      
        <li><a href="/pages/projects.html">Archive</a></li>
      
        <li class="next"><a href="/2018/11/21/Hello-Tensorflow" title="Hello Tensorflow: what is Tensorflow and how do I use it for machine learning?">Next &rarr;</a></li>
      
      </ul>
    </div>
    <hr>
    
  </div>
</div>


      </div>
      <hr>
      <footer>
        <p><small>
  <!-- start of Karl's footer; modify this part -->
          <a href="https://creativecommons.org/publicdomain/zero/1.0/"><img src="https://i.creativecommons.org/p/zero/1.0/88x31.png" alt="CC0"/></a> &nbsp;
          <a href="http://nadanai263.github.io/">Nadanai Laohakunakorn</a>
  <!-- end of Karl's footer; modify this part -->
        </small></p>
      </footer>

    </div>

    
  </body>
</html>

